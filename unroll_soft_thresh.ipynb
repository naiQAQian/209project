{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split\n",
    "import cv2\n",
    "import pywt\n",
    "\n",
    "\n",
    "# Helper function for loading TIFF images\n",
    "def load_tiff_images(folder_path):\n",
    "    images = []\n",
    "    for file_name in sorted(os.listdir(folder_path)):\n",
    "        if file_name.endswith(\".tif\") or file_name.endswith(\".tiff\"):\n",
    "            img = Image.open(os.path.join(folder_path, file_name))\n",
    "            images.append(np.array(img, dtype=np.float32))\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_mask(height, width, center_fraction=0.1, undersample_fraction=0.1, seed=50):\n",
    "    \"\"\"\n",
    "    Create a 2D random mask with a fully sampled center region.\n",
    "\n",
    "    Args:\n",
    "        height: The height of the mask (frequency encoding dimension).\n",
    "        width: The width of the mask (phase encoding dimension).\n",
    "        center_fraction: Fraction of the image to be fully sampled at the center.\n",
    "        undersample_fraction: Fraction of the remaining k-space to be sampled randomly.\n",
    "\n",
    "    Returns:\n",
    "        mask_2d: A binary mask with the same shape as the k-space data.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)  # Set the random seed locally\n",
    "        mask = np.zeros((height, width))\n",
    "    \n",
    "    # Fully sample the center region\n",
    "    center_height = int(height * center_fraction)\n",
    "    center_width = int(width * center_fraction)\n",
    "    center_start_h = (height - center_height) // 2\n",
    "    center_start_w = (width - center_width) // 2\n",
    "    mask[center_start_h:center_start_h + center_height, center_start_w:center_start_w + center_width] = 1\n",
    "    \n",
    "    # Randomly sample the remaining k-space\n",
    "    remaining_mask = np.random.choice([0, 1], size=(height, width), p=[undersample_fraction, 1-undersample_fraction])\n",
    "    mask = np.maximum(mask, remaining_mask)  # Ensure the center is fully sampled\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, undersampled_folder, ground_truth_folder, mask):\n",
    "        self.undersampled_images = load_tiff_images(undersampled_folder)\n",
    "        self.ground_truth_images = load_tiff_images(ground_truth_folder)\n",
    "        self.mask = mask\n",
    "        \n",
    "        # Ensure consistent shapes by filtering\n",
    "        self.filtered_undersampled_images = []\n",
    "        self.filtered_ground_truth_images = []\n",
    "\n",
    "        for undersampled, ground_truth in zip(self.undersampled_images, self.ground_truth_images):\n",
    "            if ground_truth.shape == (640, 320) and undersampled.shape == (640, 320):\n",
    "                self.filtered_undersampled_images.append(undersampled)\n",
    "                self.filtered_ground_truth_images.append(ground_truth)\n",
    "\n",
    "        print(f\"Filtered dataset contains {len(self.filtered_undersampled_images)} samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filtered_undersampled_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        undersampled = self.filtered_undersampled_images[idx]\n",
    "        ground_truth = self.filtered_ground_truth_images[idx]\n",
    "        \n",
    "        # Normalize images\n",
    "        undersampled = undersampled / np.max(undersampled)\n",
    "        ground_truth = ground_truth / np.max(ground_truth)\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(undersampled, dtype=torch.float32),  # Apply mask\n",
    "            torch.tensor(ground_truth, dtype=torch.float32),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset contains 2049 samples.\n",
      "Epoch [1/10], Training Loss: 0.0056799890\n",
      "Epoch [1/10], Validation Loss: 0.0030325657\n",
      "Epoch [2/10], Training Loss: 0.0021798535\n",
      "Epoch [2/10], Validation Loss: 0.0016191164\n",
      "Epoch [3/10], Training Loss: 0.0012776266\n",
      "Epoch [3/10], Validation Loss: 0.0010478590\n",
      "Epoch [4/10], Training Loss: 0.0008960780\n",
      "Epoch [4/10], Validation Loss: 0.0008074257\n",
      "Epoch [5/10], Training Loss: 0.0007592719\n",
      "Epoch [5/10], Validation Loss: 0.0007368509\n",
      "Epoch [6/10], Training Loss: 0.0007321534\n",
      "Epoch [6/10], Validation Loss: 0.0007267777\n",
      "Epoch [7/10], Training Loss: 0.0007332960\n",
      "Epoch [7/10], Validation Loss: 0.0007271129\n",
      "Epoch [8/10], Training Loss: 0.0007349333\n",
      "Epoch [8/10], Validation Loss: 0.0007273794\n",
      "Epoch [9/10], Training Loss: 0.0007353226\n",
      "Epoch [9/10], Validation Loss: 0.0007276939\n",
      "Epoch [10/10], Training Loss: 0.0007343070\n",
      "Epoch [10/10], Validation Loss: 0.0007290748\n",
      "Processing test sample 0\n",
      "Processing test sample 1\n",
      "Processing test sample 2\n",
      "Processing test sample 3\n",
      "Processing test sample 4\n",
      "Processing test sample 5\n",
      "Processing test sample 6\n",
      "Processing test sample 7\n",
      "Processing test sample 8\n",
      "Processing test sample 9\n",
      "Processing test sample 10\n",
      "Processing test sample 11\n",
      "Processing test sample 12\n",
      "Processing test sample 13\n",
      "Processing test sample 14\n",
      "Processing test sample 15\n",
      "Processing test sample 16\n",
      "Processing test sample 17\n",
      "Processing test sample 18\n",
      "Processing test sample 19\n",
      "Processing test sample 20\n",
      "Processing test sample 21\n",
      "Processing test sample 22\n",
      "Processing test sample 23\n",
      "Processing test sample 24\n",
      "Processing test sample 25\n",
      "Processing test sample 26\n",
      "Processing test sample 27\n",
      "Processing test sample 28\n",
      "Processing test sample 29\n",
      "Processing test sample 30\n",
      "Processing test sample 31\n",
      "Processing test sample 32\n",
      "Processing test sample 33\n",
      "Processing test sample 34\n",
      "Processing test sample 35\n",
      "Processing test sample 36\n",
      "Processing test sample 37\n",
      "Processing test sample 38\n",
      "Processing test sample 39\n",
      "Processing test sample 40\n",
      "Processing test sample 41\n",
      "Processing test sample 42\n",
      "Processing test sample 43\n",
      "Processing test sample 44\n",
      "Processing test sample 45\n",
      "Processing test sample 46\n",
      "Processing test sample 47\n",
      "Processing test sample 48\n",
      "Processing test sample 49\n",
      "Processing test sample 50\n",
      "Processing test sample 51\n",
      "Processing test sample 52\n",
      "Processing test sample 53\n",
      "Processing test sample 54\n",
      "Processing test sample 55\n",
      "Processing test sample 56\n",
      "Processing test sample 57\n",
      "Processing test sample 58\n",
      "Processing test sample 59\n",
      "Processing test sample 60\n",
      "Processing test sample 61\n",
      "Processing test sample 62\n",
      "Processing test sample 63\n",
      "Processing test sample 64\n",
      "Processing test sample 65\n",
      "Processing test sample 66\n",
      "Processing test sample 67\n",
      "Processing test sample 68\n",
      "Processing test sample 69\n",
      "Processing test sample 70\n",
      "Processing test sample 71\n",
      "Processing test sample 72\n",
      "Processing test sample 73\n",
      "Processing test sample 74\n",
      "Processing test sample 75\n",
      "Processing test sample 76\n",
      "Processing test sample 77\n",
      "Processing test sample 78\n",
      "Processing test sample 79\n",
      "Processing test sample 80\n",
      "Processing test sample 81\n",
      "Processing test sample 82\n",
      "Processing test sample 83\n",
      "Processing test sample 84\n",
      "Processing test sample 85\n",
      "Processing test sample 86\n",
      "Processing test sample 87\n",
      "Processing test sample 88\n",
      "Processing test sample 89\n",
      "Processing test sample 90\n",
      "Processing test sample 91\n",
      "Processing test sample 92\n",
      "Processing test sample 93\n",
      "Processing test sample 94\n",
      "Processing test sample 95\n",
      "Processing test sample 96\n",
      "Processing test sample 97\n",
      "Processing test sample 98\n",
      "Processing test sample 99\n",
      "Processing test sample 100\n",
      "Processing test sample 101\n",
      "Processing test sample 102\n",
      "Processing test sample 103\n",
      "Processing test sample 104\n",
      "Processing test sample 105\n",
      "Processing test sample 106\n",
      "Processing test sample 107\n",
      "Processing test sample 108\n",
      "Processing test sample 109\n",
      "Processing test sample 110\n",
      "Processing test sample 111\n",
      "Processing test sample 112\n",
      "Processing test sample 113\n",
      "Processing test sample 114\n",
      "Processing test sample 115\n",
      "Processing test sample 116\n",
      "Processing test sample 117\n",
      "Processing test sample 118\n",
      "Processing test sample 119\n",
      "Processing test sample 120\n",
      "Processing test sample 121\n",
      "Processing test sample 122\n",
      "Processing test sample 123\n",
      "Processing test sample 124\n",
      "Processing test sample 125\n",
      "Processing test sample 126\n",
      "Processing test sample 127\n",
      "Processing test sample 128\n",
      "Processing test sample 129\n",
      "Processing test sample 130\n",
      "Processing test sample 131\n",
      "Processing test sample 132\n",
      "Processing test sample 133\n",
      "Processing test sample 134\n",
      "Processing test sample 135\n",
      "Processing test sample 136\n",
      "Processing test sample 137\n",
      "Processing test sample 138\n",
      "Processing test sample 139\n",
      "Processing test sample 140\n",
      "Processing test sample 141\n",
      "Processing test sample 142\n",
      "Processing test sample 143\n",
      "Processing test sample 144\n",
      "Processing test sample 145\n",
      "Processing test sample 146\n",
      "Processing test sample 147\n",
      "Processing test sample 148\n",
      "Processing test sample 149\n",
      "Processing test sample 150\n",
      "Processing test sample 151\n",
      "Processing test sample 152\n",
      "Processing test sample 153\n",
      "Processing test sample 154\n",
      "Processing test sample 155\n",
      "Processing test sample 156\n",
      "Processing test sample 157\n",
      "Processing test sample 158\n",
      "Processing test sample 159\n",
      "Processing test sample 160\n",
      "Processing test sample 161\n",
      "Processing test sample 162\n",
      "Processing test sample 163\n",
      "Processing test sample 164\n",
      "Processing test sample 165\n",
      "Processing test sample 166\n",
      "Processing test sample 167\n",
      "Processing test sample 168\n",
      "Processing test sample 169\n",
      "Processing test sample 170\n",
      "Processing test sample 171\n",
      "Processing test sample 172\n",
      "Processing test sample 173\n",
      "Processing test sample 174\n",
      "Processing test sample 175\n",
      "Processing test sample 176\n",
      "Processing test sample 177\n",
      "Processing test sample 178\n",
      "Processing test sample 179\n",
      "Processing test sample 180\n",
      "Processing test sample 181\n",
      "Processing test sample 182\n",
      "Processing test sample 183\n",
      "Processing test sample 184\n",
      "Processing test sample 185\n",
      "Processing test sample 186\n",
      "Processing test sample 187\n",
      "Processing test sample 188\n",
      "Processing test sample 189\n",
      "Processing test sample 190\n",
      "Processing test sample 191\n",
      "Processing test sample 192\n",
      "Processing test sample 193\n",
      "Processing test sample 194\n",
      "Processing test sample 195\n",
      "Processing test sample 196\n",
      "Processing test sample 197\n",
      "Processing test sample 198\n",
      "Processing test sample 199\n",
      "Processing test sample 200\n",
      "Processing test sample 201\n",
      "Processing test sample 202\n",
      "Processing test sample 203\n",
      "Processing test sample 204\n",
      "Processing test sample 205\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class VSQPReconstructionNetwork(nn.Module):\n",
    "    def __init__(self, num_layers=5):\n",
    "        super(VSQPReconstructionNetwork, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.mu = nn.Parameter(torch.tensor(0.1))  # Learnable penalty parameter\n",
    "        \n",
    "        # Separate learnable thresholds for real and imaginary parts\n",
    "        self.regularizers_real = nn.ParameterList([\n",
    "            nn.Parameter(torch.tensor(0.1)) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.regularizers_imag = nn.ParameterList([\n",
    "            nn.Parameter(torch.tensor(0.1)) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "    def apply_wavelet_soft_threshold(self, x, threshold_real, threshold_imag):\n",
    "        \"\"\"\n",
    "        Applies wavelet soft thresholding separately to the real and imaginary parts of the input.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Complex-valued input tensor of shape [batch, height, width].\n",
    "            threshold_real (torch.Tensor): Threshold for the real part.\n",
    "            threshold_imag (torch.Tensor): Threshold for the imaginary part.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Complex-valued tensor after wavelet thresholding.\n",
    "        \"\"\"\n",
    "        # Extract real and imaginary parts\n",
    "        x_real = x.real.detach().cpu().numpy()\n",
    "        x_imag = x.imag.detach().cpu().numpy()\n",
    "\n",
    "        # Apply wavelet decomposition and thresholding to the real part\n",
    "        coeffs_real = pywt.wavedec2(x_real, wavelet='haar')\n",
    "        thresholded_coeffs_real = [\n",
    "            pywt.threshold(c, threshold_real.item(), mode='soft') if isinstance(c, np.ndarray) else c\n",
    "            for c in coeffs_real\n",
    "        ]\n",
    "        x_real_thresholded = pywt.waverec2(thresholded_coeffs_real, wavelet='haar')\n",
    "\n",
    "        # Apply wavelet decomposition and thresholding to the imaginary part\n",
    "        coeffs_imag = pywt.wavedec2(x_imag, wavelet='haar')\n",
    "        thresholded_coeffs_imag = [\n",
    "            pywt.threshold(c, threshold_imag.item(), mode='soft') if isinstance(c, np.ndarray) else c\n",
    "            for c in coeffs_imag\n",
    "        ]\n",
    "        x_imag_thresholded = pywt.waverec2(thresholded_coeffs_imag, wavelet='haar')\n",
    "\n",
    "        # Recombine the real and imaginary parts into a complex tensor\n",
    "        x_thresholded = torch.tensor(\n",
    "            x_real_thresholded + 1j * x_imag_thresholded,\n",
    "            device=x.device,\n",
    "            dtype=torch.complex64\n",
    "        )\n",
    "\n",
    "        return x_thresholded\n",
    "    \n",
    "    def forward(self, x_init, y, mask):\n",
    "        \"\"\"\n",
    "        x_init: Initial estimate of the image (image space)\n",
    "        y: Undersampled k-space data (in image space, transformed to k-space)\n",
    "        mask: Sampling mask (applied in k-space)\n",
    "        \"\"\"\n",
    "        x = x_init\n",
    "        y_k = torch.fft.fftshift(torch.fft.fft2(torch.fft.ifftshift(y)))  # Transform y to k-space and apply fftshift\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            # Regularization step (proximal operator - soft-thresholding)\n",
    "            x = x +0j\n",
    "            x_real = x.real\n",
    "            x_imag = x.imag\n",
    "            x_complex = torch.complex(x_real, x_imag)\n",
    "            z = self.apply_wavelet_soft_threshold(x_complex, self.regularizers_real[i], self.regularizers_imag[i])\n",
    "\n",
    "            # Data consistency step\n",
    "            # Step 1: Forward model (apply mask in k-space)\n",
    "            x_k = torch.fft.fftshift(torch.fft.fft2(torch.fft.ifftshift(x)))  # Transform x to k-space and apply fftshift\n",
    "            Ax = mask * x_k  # Apply k-space mask\n",
    "            \n",
    "            # Step 2: Residual in k-space\n",
    "            residual_k = y_k - Ax\n",
    "            \n",
    "            # Step 3: Backproject residual (to image space)\n",
    "            residual = torch.fft.fftshift(torch.fft.ifft2(torch.fft.ifftshift(residual_k)))  # Inverse FT to image space\n",
    "            #residual = torch.abs(residual)\n",
    "\n",
    "            # Combine regularized and data consistency updates\n",
    "            x = self.mu * z + residual\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_network(model, train_dataloader, val_dataloader, num_epochs, learning_rate, device, loss_ratio = 0.5):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for undersampled, ground_truth in train_dataloader:\n",
    "            undersampled = undersampled.to(device)\n",
    "            ground_truth = ground_truth.to(device)           \n",
    "            ground_truth_k = torch.fft.fftshift(torch.fft.fft2(ground_truth))  # Fully-sampled k-space\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(undersampled, undersampled, mask=torch.tensor(mask_2d, dtype=torch.float32).to(device))\n",
    "            output_k = torch.fft.fftshift(torch.fft.fft2(output))  # Fully-sampled k-space\n",
    "            \n",
    "            ground_truth_k = ground_truth_k / torch.abs(ground_truth_k).max()\n",
    "            output_k = output_k / torch.abs(output_k).max()\n",
    "\n",
    "            loss = loss_fn(torch.abs(output), torch.abs(ground_truth))\n",
    "            #loss = loss_fn(torch.log1p(output_k.abs()), torch.log1p(ground_truth_k.abs()))\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_loss /= len(train_dataloader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Training Loss: {train_loss:.10f}\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for undersampled, ground_truth in val_dataloader:\n",
    "                undersampled = undersampled.to(device)\n",
    "                ground_truth = ground_truth.to(device)\n",
    "                ground_truth_k = torch.fft.fftshift(torch.fft.fft2(ground_truth))  # Fully-sampled k-space               \n",
    "                output = model(undersampled, undersampled, mask=torch.tensor(mask_2d, dtype=torch.float32).to(device))\n",
    "                output_k = torch.fft.fftshift(torch.fft.fft2(output))  # Reconstructed k-space\n",
    "                \n",
    "                ground_truth_k = ground_truth_k / torch.abs(ground_truth_k).max()\n",
    "                output_k = output_k / torch.abs(output_k).max()            \n",
    "                \n",
    "                loss = loss_fn(torch.abs(output), torch.abs(ground_truth))\n",
    "            \n",
    "                #loss = loss_fn(torch.log1p(output_k.abs()), torch.log1p(ground_truth_k.abs()))\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_dataloader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {val_loss:.10f}\")\n",
    "\n",
    "            \n",
    "def save_results_as_numpy(undersampled, output, ground_truth, index, save_dir=\"results\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "    \n",
    "    np.save(os.path.join(save_dir, f\"undersampled_{index}.npy\"), undersampled)\n",
    "    np.save(os.path.join(save_dir, f\"output_{index}.npy\"), output)\n",
    "    np.save(os.path.join(save_dir, f\"ground_truth_{index}.npy\"), ground_truth)\n",
    "    #print(f\"Results for sample {index} saved in {save_dir}\")\n",
    "\n",
    "            \n",
    "def process_and_save_all_test_images(model, test_dataloader, device, save_dir=\"results\"):\n",
    "    model.eval()\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Ensure the save directory exists\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (undersampled, ground_truth) in enumerate(test_dataloader):\n",
    "            print(f\"Processing test sample {i}\")\n",
    "\n",
    "            # Move data to device\n",
    "            undersampled = undersampled.to(device)\n",
    "            ground_truth = ground_truth.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(undersampled, undersampled, mask=torch.tensor(mask_2d, dtype=torch.float32).to(device))\n",
    "\n",
    "            # Convert tensors to numpy arrays\n",
    "            undersampled_np = undersampled[0].cpu().numpy()  # Move to CPU and convert to NumPy\n",
    "            output_np = output[0].cpu().numpy()  # Move to CPU and convert to NumPy\n",
    "            ground_truth_np = ground_truth[0].cpu().numpy()  # Move to CPU and convert to NumPy\n",
    "\n",
    "            # Save the results\n",
    "            save_results_as_numpy(undersampled_np, output_np, ground_truth_np, index=i, save_dir=save_dir)\n",
    "\n",
    "            #print(f\"Saved test sample {i} to {save_dir}\")\n",
    "            \n",
    "            # Free memory\n",
    "            del undersampled, ground_truth, output\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "        # Define paths\n",
    "    undersampled_folder = r\"D:\\Class Project\\209\\brain_multicoil_train_batch_1\\noisy_images_0.75\"\n",
    "    #undersampled_folder = r\"D:\\Class Project\\209\\brain_multicoil_train_batch_1\\noisy_images\"\n",
    "    ground_truth_folder = r\"D:\\Class Project\\209\\brain_multicoil_train_batch_1\\ground_truth_images\"\n",
    "    #undersampled_folder = r\"F:\\Class Project\\data\\ground_truth_images\"\n",
    "    #ground_truth_folder = r\"F:\\Class Project\\data\\noisy_images_0.1\"\n",
    "    \n",
    "    # Create mask\n",
    "    height, width = 640, 320  # Example dimensions (adjust as needed)\n",
    "    np.random.seed(50)\n",
    "    mask_2d = create_random_mask(height, width, center_fraction=0.2, undersample_fraction=0.75, seed=50)# this need to match the mask used to generate data!!!\n",
    "    \n",
    "    # Define dataset\n",
    "    dataset = MRIDataset(undersampled_folder, ground_truth_folder, mask_2d)\n",
    "    \n",
    "    # Split dataset\n",
    "    train_size = int(0.7 * len(dataset))\n",
    "    val_size = int(0.2 * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = VSQPReconstructionNetwork(num_layers=5)\n",
    "    \n",
    "    # Train model\n",
    "    train_network(model, train_dataloader, val_dataloader, num_epochs=10, learning_rate=0.001, device=device)\n",
    "    \n",
    "    # Test and visualize\n",
    "    #test_network(model, test_dataloader, device=device)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    #evaluate_network(model, dataloader, device=device)\n",
    "    process_and_save_all_test_images(model, test_dataloader, device=device, save_dir=\"results\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
