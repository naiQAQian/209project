{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split\n",
    "import cv2\n",
    "import pywt\n",
    "\n",
    "\n",
    "# Helper function for loading TIFF images\n",
    "def load_tiff_images(folder_path):\n",
    "    images = []\n",
    "    for file_name in sorted(os.listdir(folder_path)):\n",
    "        if file_name.endswith(\".tif\") or file_name.endswith(\".tiff\"):\n",
    "            img = Image.open(os.path.join(folder_path, file_name))\n",
    "            images.append(np.array(img, dtype=np.float32))\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_mask(height, width, center_fraction=0.1, undersample_fraction=0.1, seed=50):\n",
    "    \"\"\"\n",
    "    Create a 2D random mask with a fully sampled center region.\n",
    "\n",
    "    Args:\n",
    "        height: The height of the mask (frequency encoding dimension).\n",
    "        width: The width of the mask (phase encoding dimension).\n",
    "        center_fraction: Fraction of the image to be fully sampled at the center.\n",
    "        undersample_fraction: Fraction of the remaining k-space to be sampled randomly.\n",
    "\n",
    "    Returns:\n",
    "        mask_2d: A binary mask with the same shape as the k-space data.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)  # Set the random seed locally\n",
    "        mask = np.zeros((height, width))\n",
    "    \n",
    "    # Fully sample the center region\n",
    "    center_height = int(height * center_fraction)\n",
    "    center_width = int(width * center_fraction)\n",
    "    center_start_h = (height - center_height) // 2\n",
    "    center_start_w = (width - center_width) // 2\n",
    "    mask[center_start_h:center_start_h + center_height, center_start_w:center_start_w + center_width] = 1\n",
    "    \n",
    "    # Randomly sample the remaining k-space\n",
    "    remaining_mask = np.random.choice([0, 1], size=(height, width), p=[undersample_fraction, 1-undersample_fraction])\n",
    "    mask = np.maximum(mask, remaining_mask)  # Ensure the center is fully sampled\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, undersampled_folder, ground_truth_folder, mask):\n",
    "        self.undersampled_images = load_tiff_images(undersampled_folder)\n",
    "        self.ground_truth_images = load_tiff_images(ground_truth_folder)\n",
    "        self.mask = mask\n",
    "        \n",
    "        # Ensure consistent shapes by filtering\n",
    "        self.filtered_undersampled_images = []\n",
    "        self.filtered_ground_truth_images = []\n",
    "\n",
    "        for undersampled, ground_truth in zip(self.undersampled_images, self.ground_truth_images):\n",
    "            if ground_truth.shape == (640, 320) and undersampled.shape == (640, 320):\n",
    "                self.filtered_undersampled_images.append(undersampled)\n",
    "                self.filtered_ground_truth_images.append(ground_truth)\n",
    "\n",
    "        print(f\"Filtered dataset contains {len(self.filtered_undersampled_images)} samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filtered_undersampled_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        undersampled = self.filtered_undersampled_images[idx]\n",
    "        ground_truth = self.filtered_ground_truth_images[idx]\n",
    "        \n",
    "        # Normalize images\n",
    "        undersampled = undersampled / np.max(undersampled)\n",
    "        ground_truth = ground_truth / np.max(ground_truth)\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(undersampled, dtype=torch.float32),  # Apply mask\n",
    "            torch.tensor(ground_truth, dtype=torch.float32),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset contains 2049 samples.\n",
      "Epoch [1/20], Training Loss: 0.2774887845\n",
      "Epoch [1/20], Validation Loss: 0.2263909994\n",
      "Epoch [2/20], Training Loss: 0.2397465750\n",
      "Epoch [2/20], Validation Loss: 0.2319755329\n",
      "Epoch [3/20], Training Loss: 0.2439987736\n",
      "Epoch [3/20], Validation Loss: 0.2325923015\n",
      "Epoch [4/20], Training Loss: 0.2440936961\n",
      "Epoch [4/20], Validation Loss: 0.2329993158\n",
      "Epoch [5/20], Training Loss: 0.2443499697\n",
      "Epoch [5/20], Validation Loss: 0.2326943576\n",
      "Epoch [6/20], Training Loss: 0.2442412279\n",
      "Epoch [6/20], Validation Loss: 0.2327189903\n",
      "Epoch [7/20], Training Loss: 0.2442833703\n",
      "Epoch [7/20], Validation Loss: 0.2328094497\n",
      "Epoch [8/20], Training Loss: 0.2442770720\n",
      "Epoch [8/20], Validation Loss: 0.2328458433\n",
      "Epoch [9/20], Training Loss: 0.2444667960\n",
      "Epoch [9/20], Validation Loss: 0.2331820394\n",
      "Epoch [10/20], Training Loss: 0.2444678399\n",
      "Epoch [10/20], Validation Loss: 0.2323507711\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 187\u001b[0m\n\u001b[0;32m    184\u001b[0m model \u001b[38;5;241m=\u001b[39m VSQPReconstructionNetwork(num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m train_network(model, train_dataloader, val_dataloader, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Test and visualize\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m#test_network(model, test_dataloader, device=device)\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m#evaluate_network(model, dataloader, device=device)\u001b[39;00m\n\u001b[0;32m    194\u001b[0m process_and_save_all_test_images(model, test_dataloader, device\u001b[38;5;241m=\u001b[39mdevice, save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 79\u001b[0m, in \u001b[0;36mtrain_network\u001b[1;34m(model, train_dataloader, val_dataloader, num_epochs, learning_rate, device, loss_ratio)\u001b[0m\n\u001b[0;32m     76\u001b[0m ground_truth_k \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfftshift(torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfft2(ground_truth))  \u001b[38;5;66;03m# Fully-sampled k-space\u001b[39;00m\n\u001b[0;32m     78\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 79\u001b[0m output \u001b[38;5;241m=\u001b[39m model(undersampled, undersampled, mask\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(mask_2d, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m     80\u001b[0m output_k \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfftshift(torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfft2(output))  \u001b[38;5;66;03m# Fully-sampled k-space\u001b[39;00m\n\u001b[0;32m     82\u001b[0m ground_truth_k \u001b[38;5;241m=\u001b[39m ground_truth_k \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(ground_truth_k)\u001b[38;5;241m.\u001b[39mmax()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[4], line 23\u001b[0m, in \u001b[0;36mVSQPReconstructionNetwork.forward\u001b[1;34m(self, x_init, y, mask)\u001b[0m\n\u001b[0;32m     19\u001b[0m y_k \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfftshift(torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfft2(y))  \u001b[38;5;66;03m# Transform y to k-space and apply fftshift\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Regularization step (proximal operator - soft-thresholding)\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_wavelet_soft_threshold(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregularizers[i])\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Data consistency step\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Step 1: Forward model (apply mask in k-space)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     x_k \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfftshift(torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfft2(x))  \u001b[38;5;66;03m# Transform x to k-space and apply fftshift\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 56\u001b[0m, in \u001b[0;36mVSQPReconstructionNetwork.apply_wavelet_soft_threshold\u001b[1;34m(self, x, threshold)\u001b[0m\n\u001b[0;32m     50\u001b[0m thresholded_coeffs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     51\u001b[0m     pywt\u001b[38;5;241m.\u001b[39mthreshold(c, threshold\u001b[38;5;241m.\u001b[39mitem(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoft\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m c\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m coeffs\n\u001b[0;32m     53\u001b[0m ]\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Wavelet reconstruction\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m x_thresholded \u001b[38;5;241m=\u001b[39m pywt\u001b[38;5;241m.\u001b[39mwaverec2(thresholded_coeffs, wavelet\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhaar\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Convert back to a PyTorch tensor\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(x_thresholded, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pywt\\_multilevel.py:335\u001b[0m, in \u001b[0;36mwaverec2\u001b[1;34m(coeffs, wavelet, mode, axes)\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll detail shapes must be the same length.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    333\u001b[0m         idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m a_len \u001b[38;5;241m==\u001b[39m d_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    334\u001b[0m                      \u001b[38;5;28;01mfor\u001b[39;00m a_len, d_len \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(a\u001b[38;5;241m.\u001b[39mshape, d_shape))\n\u001b[1;32m--> 335\u001b[0m     a \u001b[38;5;241m=\u001b[39m idwt2((a[idxs], d), wavelet, mode, axes)\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pywt\\_multidim.py:118\u001b[0m, in \u001b[0;36midwt2\u001b[1;34m(coeffs, wavelet, mode, axes)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2 axes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    117\u001b[0m coeffs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maa\u001b[39m\u001b[38;5;124m'\u001b[39m: LL, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mda\u001b[39m\u001b[38;5;124m'\u001b[39m: HL, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mad\u001b[39m\u001b[38;5;124m'\u001b[39m: LH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdd\u001b[39m\u001b[38;5;124m'\u001b[39m: HH}\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m idwtn(coeffs, wavelet, mode, axes)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pywt\\_multidim.py:311\u001b[0m, in \u001b[0;36midwtn\u001b[1;34m(coeffs, wavelet, mode, axes)\u001b[0m\n\u001b[0;32m    309\u001b[0m                 L \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(L, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    310\u001b[0m                 H \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(H, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m--> 311\u001b[0m         new_coeffs[key] \u001b[38;5;241m=\u001b[39m idwt_axis(L, H, wav, mode, axis)\n\u001b[0;32m    312\u001b[0m     coeffs \u001b[38;5;241m=\u001b[39m new_coeffs\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m coeffs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "class VSQPReconstructionNetwork(nn.Module):\n",
    "    def __init__(self, num_layers=5):\n",
    "        super(VSQPReconstructionNetwork, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.mu = nn.Parameter(torch.tensor(0.1))  # Learnable penalty parameter\n",
    "        \n",
    "        # Use nn.ParameterList to store the regularization thresholds\n",
    "        self.regularizers = nn.ParameterList([\n",
    "            nn.Parameter(torch.tensor(0.1)) for _ in range(num_layers)  # Thresholds for soft-thresholding\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x_init, y, mask):\n",
    "        \"\"\"\n",
    "        x_init: Initial estimate of the image (image space)\n",
    "        y: Undersampled k-space data (in image space, transformed to k-space)\n",
    "        mask: Sampling mask (applied in k-space)\n",
    "        \"\"\"\n",
    "        x = x_init\n",
    "        y_k = torch.fft.fftshift(torch.fft.fft2(y))  # Transform y to k-space and apply fftshift\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            # Regularization step (proximal operator - soft-thresholding)\n",
    "            z = self.apply_wavelet_soft_threshold(x, self.regularizers[i])\n",
    "\n",
    "            # Data consistency step\n",
    "            # Step 1: Forward model (apply mask in k-space)\n",
    "            x_k = torch.fft.fftshift(torch.fft.fft2(x))  # Transform x to k-space and apply fftshift\n",
    "            Ax = mask * x_k  # Apply k-space mask\n",
    "            \n",
    "            # Step 2: Residual in k-space\n",
    "            residual_k = y_k - Ax\n",
    "            \n",
    "            # Step 3: Backproject residual (to image space)\n",
    "            residual_k_shifted = mask * residual_k  # Apply mask again (if needed)\n",
    "            residual = torch.fft.ifftshift(torch.fft.ifft2(torch.fft.ifftshift(residual_k_shifted)).real)  # Inverse FT to image space\n",
    "\n",
    "            # Combine regularized and data consistency updates\n",
    "            x = self.mu * z + residual\n",
    "\n",
    "        return x\n",
    "\n",
    "    def apply_wavelet_soft_threshold(self, x, threshold):\n",
    "        # Detach and move the tensor to CPU before converting to NumPy\n",
    "        x_np = x.detach().cpu().numpy()\n",
    "        \n",
    "        # Wavelet decomposition\n",
    "        coeffs = pywt.wavedec2(x_np, wavelet='haar')\n",
    "        \n",
    "        # Apply soft thresholding to coefficients\n",
    "        thresholded_coeffs = [\n",
    "            pywt.threshold(c, threshold.item(), mode='soft') if isinstance(c, np.ndarray) else c\n",
    "            for c in coeffs\n",
    "        ]\n",
    "        \n",
    "        # Wavelet reconstruction\n",
    "        x_thresholded = pywt.waverec2(thresholded_coeffs, wavelet='haar')\n",
    "        \n",
    "        # Convert back to a PyTorch tensor\n",
    "        return torch.tensor(x_thresholded, device=x.device, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_network(model, train_dataloader, val_dataloader, num_epochs, learning_rate, device, loss_ratio = 0.5):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for undersampled, ground_truth in train_dataloader:\n",
    "            undersampled = undersampled.to(device)\n",
    "            ground_truth = ground_truth.to(device)           \n",
    "            ground_truth_k = torch.fft.fftshift(torch.fft.fft2(ground_truth))  # Fully-sampled k-space\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(undersampled, undersampled, mask=torch.tensor(mask_2d, dtype=torch.float32).to(device))\n",
    "            output_k = torch.fft.fftshift(torch.fft.fft2(output))  # Fully-sampled k-space\n",
    "            \n",
    "            ground_truth_k = ground_truth_k / torch.abs(ground_truth_k).max()\n",
    "            output_k = output_k / torch.abs(output_k).max()\n",
    "\n",
    "            loss = loss_fn(output, ground_truth)\n",
    "            #loss = loss_fn(torch.log1p(output_k.abs()), torch.log1p(ground_truth_k.abs()))\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_loss /= len(train_dataloader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Training Loss: {train_loss:.10f}\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for undersampled, ground_truth in val_dataloader:\n",
    "                undersampled = undersampled.to(device)\n",
    "                ground_truth = ground_truth.to(device)\n",
    "                ground_truth_k = torch.fft.fftshift(torch.fft.fft2(ground_truth))  # Fully-sampled k-space               \n",
    "                output = model(undersampled, undersampled, mask=torch.tensor(mask_2d, dtype=torch.float32).to(device))\n",
    "                output_k = torch.fft.fftshift(torch.fft.fft2(output))  # Reconstructed k-space\n",
    "                \n",
    "                ground_truth_k = ground_truth_k / torch.abs(ground_truth_k).max()\n",
    "                output_k = output_k / torch.abs(output_k).max()            \n",
    "                \n",
    "                loss = loss_fn(output, ground_truth)\n",
    "            \n",
    "                #loss = loss_fn(torch.log1p(output_k.abs()), torch.log1p(ground_truth_k.abs()))\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_dataloader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {val_loss:.10f}\")\n",
    "\n",
    "            \n",
    "def save_results_as_numpy(undersampled, output, ground_truth, index, save_dir=\"results\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "    \n",
    "    np.save(os.path.join(save_dir, f\"undersampled_{index}.npy\"), undersampled)\n",
    "    np.save(os.path.join(save_dir, f\"output_{index}.npy\"), output)\n",
    "    np.save(os.path.join(save_dir, f\"ground_truth_{index}.npy\"), ground_truth)\n",
    "    #print(f\"Results for sample {index} saved in {save_dir}\")\n",
    "\n",
    "            \n",
    "def process_and_save_all_test_images(model, test_dataloader, device, save_dir=\"results\"):\n",
    "    model.eval()\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Ensure the save directory exists\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (undersampled, ground_truth) in enumerate(test_dataloader):\n",
    "            print(f\"Processing test sample {i}\")\n",
    "\n",
    "            # Move data to device\n",
    "            undersampled = undersampled.to(device)\n",
    "            ground_truth = ground_truth.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(undersampled, undersampled, mask=torch.tensor(mask_2d, dtype=torch.float32).to(device))\n",
    "\n",
    "            # Convert tensors to numpy arrays\n",
    "            undersampled_np = undersampled[0].cpu().numpy()  # Move to CPU and convert to NumPy\n",
    "            output_np = output[0].cpu().numpy()  # Move to CPU and convert to NumPy\n",
    "            ground_truth_np = ground_truth[0].cpu().numpy()  # Move to CPU and convert to NumPy\n",
    "\n",
    "            # Save the results\n",
    "            save_results_as_numpy(undersampled_np, output_np, ground_truth_np, index=i, save_dir=save_dir)\n",
    "\n",
    "            #print(f\"Saved test sample {i} to {save_dir}\")\n",
    "            \n",
    "            # Free memory\n",
    "            del undersampled, ground_truth, output\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "        # Define paths\n",
    "    undersampled_folder = r\"D:\\Class Project\\209\\brain_multicoil_train_batch_1\\noisy_images_0.1\"\n",
    "    ground_truth_folder = r\"D:\\Class Project\\209\\brain_multicoil_train_batch_1\\ground_truth_images\"\n",
    "    \n",
    "    # Create mask\n",
    "    height, width = 640, 320  # Example dimensions (adjust as needed)\n",
    "    np.random.seed(50)\n",
    "    mask_2d = create_random_mask(height, width, center_fraction=0.1, undersample_fraction=0.3, seed=50)# this need to match the mask used to generate data!!!\n",
    "    \n",
    "    # Define dataset\n",
    "    dataset = MRIDataset(undersampled_folder, ground_truth_folder, mask_2d)\n",
    "    \n",
    "    # Split dataset\n",
    "    train_size = int(0.7 * len(dataset))\n",
    "    val_size = int(0.2 * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = VSQPReconstructionNetwork(num_layers=5)\n",
    "    \n",
    "    # Train model\n",
    "    train_network(model, train_dataloader, val_dataloader, num_epochs=20, learning_rate=0.001, device=device)\n",
    "    \n",
    "    # Test and visualize\n",
    "    #test_network(model, test_dataloader, device=device)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    #evaluate_network(model, dataloader, device=device)\n",
    "    process_and_save_all_test_images(model, test_dataloader, device=device, save_dir=\"results\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
