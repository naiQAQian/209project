{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split\n",
    "import cv2\n",
    "import pywt\n",
    "\n",
    "\n",
    "# Helper function for loading TIFF images\n",
    "def load_tiff_images(folder_path):\n",
    "    images = []\n",
    "    for file_name in sorted(os.listdir(folder_path)):\n",
    "        if file_name.endswith(\".tif\") or file_name.endswith(\".tiff\"):\n",
    "            img = Image.open(os.path.join(folder_path, file_name))\n",
    "            images.append(np.array(img, dtype=np.float32))\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, undersampled_folder, ground_truth_folder, mask):\n",
    "        self.undersampled_images = load_tiff_images(undersampled_folder)\n",
    "        self.ground_truth_images = load_tiff_images(ground_truth_folder)\n",
    "        self.mask = mask\n",
    "        \n",
    "        # Ensure consistent shapes by filtering\n",
    "        self.filtered_undersampled_images = []\n",
    "        self.filtered_ground_truth_images = []\n",
    "\n",
    "        for undersampled, ground_truth in zip(self.undersampled_images, self.ground_truth_images):\n",
    "            if ground_truth.shape == (640, 320) and undersampled.shape == (640, 320):\n",
    "                self.filtered_undersampled_images.append(undersampled)\n",
    "                self.filtered_ground_truth_images.append(ground_truth)\n",
    "\n",
    "        print(f\"Filtered dataset contains {len(self.filtered_undersampled_images)} samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filtered_undersampled_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        undersampled = self.filtered_undersampled_images[idx]\n",
    "        ground_truth = self.filtered_ground_truth_images[idx]\n",
    "        \n",
    "        # Normalize images\n",
    "        undersampled = undersampled / np.max(undersampled)\n",
    "        ground_truth = ground_truth / np.max(ground_truth)\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(undersampled, dtype=torch.float32),  # Apply mask\n",
    "            torch.tensor(ground_truth, dtype=torch.float32),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_mask(height, width, center_fraction=0.1, undersample_fraction=0.1, seed=50):#at least 0.5\n",
    "    \"\"\"\n",
    "    Create a 2D random mask with a fully sampled center region.\n",
    "\n",
    "    Args:\n",
    "        height: The height of the mask (frequency encoding dimension).\n",
    "        width: The width of the mask (phase encoding dimension).\n",
    "        center_fraction: Fraction of the image to be fully sampled at the center.\n",
    "        undersample_fraction: Fraction of the remaining k-space to be sampled randomly.\n",
    "\n",
    "    Returns:\n",
    "        mask_2d: A binary mask with the same shape as the k-space data.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)  # Set the random seed locally\n",
    "        mask = np.zeros((height, width))\n",
    "    \n",
    "    # Fully sample the center region\n",
    "    center_height = int(height * center_fraction)\n",
    "    center_width = int(width * center_fraction)\n",
    "    center_start_h = (height - center_height) // 2\n",
    "    center_start_w = (width - center_width) // 2\n",
    "    mask[center_start_h:center_start_h + center_height, center_start_w:center_start_w + center_width] = 1\n",
    "    \n",
    "    # Randomly sample the remaining k-space\n",
    "    remaining_mask = np.random.choice([0, 1], size=(height, width), p=[undersample_fraction, 1-undersample_fraction])\n",
    "    mask = np.maximum(mask, remaining_mask)  # Ensure the center is fully sampled\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAE_4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DAE_4, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 1, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dae_bottleneck(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(dae_bottleneck, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        bottleneck = self.bottleneck(encoded)\n",
    "        decoded = self.decoder(bottleneck)\n",
    "        return decoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDAE(nn.Module):\n",
    "    def __init__(self, input_channels=1, base_filters=64):\n",
    "        super(UNetDAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = self.conv_block(input_channels, base_filters)\n",
    "        self.enc2 = self.conv_block(base_filters, base_filters * 2)\n",
    "        self.enc3 = self.conv_block(base_filters * 2, base_filters * 4)\n",
    "        self.enc4 = self.conv_block(base_filters * 4, base_filters * 8)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.conv_block(base_filters * 8, base_filters * 16)\n",
    "        \n",
    "        # Decoder\n",
    "        self.up4 = nn.ConvTranspose2d(base_filters * 16, base_filters * 8, kernel_size=2, stride=2)\n",
    "        self.dec4 = self.conv_block(base_filters * 16, base_filters * 8)\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose2d(base_filters * 8, base_filters * 4, kernel_size=2, stride=2)\n",
    "        self.dec3 = self.conv_block(base_filters * 8, base_filters * 4)\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(base_filters * 4, base_filters * 2, kernel_size=2, stride=2)\n",
    "        self.dec2 = self.conv_block(base_filters * 4, base_filters * 2)\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(base_filters * 2, base_filters, kernel_size=2, stride=2)\n",
    "        self.dec1 = self.conv_block(base_filters * 2, base_filters)\n",
    "        \n",
    "        # Final output layer\n",
    "        self.final = nn.Conv2d(base_filters, input_channels, kernel_size=1)\n",
    "\n",
    "        # Pooling\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(self.pool(enc1))\n",
    "        enc3 = self.enc3(self.pool(enc2))\n",
    "        enc4 = self.enc4(self.pool(enc3))\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(self.pool(enc4))\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        dec4 = self.dec4(torch.cat([self.up4(bottleneck), enc4], dim=1))\n",
    "        dec3 = self.dec3(torch.cat([self.up3(dec4), enc3], dim=1))\n",
    "        dec2 = self.dec2(torch.cat([self.up2(dec3), enc2], dim=1))\n",
    "        dec1 = self.dec1(torch.cat([self.up1(dec2), enc1], dim=1))\n",
    "        \n",
    "        return self.final(dec1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset contains 2049 samples.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (struct c10::complex<float>) and bias type (float) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 156\u001b[0m\n\u001b[0;32m    153\u001b[0m model \u001b[38;5;241m=\u001b[39m VSQPReconstructionNetwork(num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m train_network(model, train_dataloader, val_dataloader, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# Test and visualize\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m#test_network(model, test_dataloader, device=device)\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m#evaluate_network(model, dataloader, device=device)\u001b[39;00m\n\u001b[0;32m    163\u001b[0m process_and_save_all_test_images(model, test_dataloader, device\u001b[38;5;241m=\u001b[39mdevice, save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 49\u001b[0m, in \u001b[0;36mtrain_network\u001b[1;34m(model, train_dataloader, val_dataloader, num_epochs, learning_rate, device)\u001b[0m\n\u001b[0;32m     46\u001b[0m ground_truth_k \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfftshift(torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfft2(ground_truth))  \u001b[38;5;66;03m# Fully-sampled k-space\u001b[39;00m\n\u001b[0;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 49\u001b[0m output \u001b[38;5;241m=\u001b[39m model(undersampled, undersampled, mask\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(mask_2d, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m     50\u001b[0m output_k \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfftshift(torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfft2(output))  \u001b[38;5;66;03m# Fully-sampled k-space\u001b[39;00m\n\u001b[0;32m     52\u001b[0m ground_truth_k \u001b[38;5;241m=\u001b[39m ground_truth_k \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(ground_truth_k)\u001b[38;5;241m.\u001b[39mmax()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[7], line 19\u001b[0m, in \u001b[0;36mVSQPReconstructionNetwork.forward\u001b[1;34m(self, x_init, y, mask)\u001b[0m\n\u001b[0;32m     15\u001b[0m y_k \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfftshift(torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfft2(torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mifftshift(y)))  \u001b[38;5;66;03m# Transform y to k-space and apply fftshift\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Regularization step using the DAE\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregularization_net(x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Data consistency step\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     x_k \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfftshift(torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfft2(torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mifftshift(x)))  \u001b[38;5;66;03m# Transform x to k-space and apply fftshift\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[6], line 43\u001b[0m, in \u001b[0;36mUNetDAE.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# Encoder\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m     enc1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc1(x)\n\u001b[0;32m     44\u001b[0m     enc2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(enc1))\n\u001b[0;32m     45\u001b[0m     enc3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(enc2))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[0;32m    551\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (struct c10::complex<float>) and bias type (float) should be the same"
     ]
    }
   ],
   "source": [
    "\n",
    "class VSQPReconstructionNetwork(nn.Module):\n",
    "    def __init__(self, num_layers=5):\n",
    "        super(VSQPReconstructionNetwork, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.mu = nn.Parameter(torch.tensor(0.1))  # Learnable penalty parameter\n",
    "        self.regularization_net = UNetDAE()  # Replace wavelet-based regularization with DAE\n",
    "\n",
    "    def forward(self, x_init, y, mask):\n",
    "        \"\"\"\n",
    "        x_init: Initial estimate of the image (image space)\n",
    "        y: Undersampled k-space data (in image space, transformed to k-space)\n",
    "        mask: Sampling mask (applied in k-space)\n",
    "        \"\"\"\n",
    "        x = x_init\n",
    "        y_k = torch.fft.fftshift(torch.fft.fft2(torch.fft.ifftshift(y)))  # Transform y to k-space and apply fftshift\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            # Regularization step using the DAE\n",
    "            z = self.regularization_net(x.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            # Data consistency step\n",
    "            x_k = torch.fft.fftshift(torch.fft.fft2(torch.fft.ifftshift(x)))  # Transform x to k-space and apply fftshift\n",
    "            Ax = mask * x_k  # Apply k-space mask\n",
    "            residual_k = y_k - Ax\n",
    "            residual = torch.fft.fftshift(torch.fft.ifft2(torch.fft.fftshift(residual_k)))  # Inverse FT to image space\n",
    "            residual = torch.abs(residual)\n",
    "            # Combine regularized and data consistency updates\n",
    "            x = self.mu * z + residual\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def train_network(model, train_dataloader, val_dataloader, num_epochs, learning_rate, device):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for undersampled, ground_truth in train_dataloader:\n",
    "            undersampled = undersampled.to(device)\n",
    "            ground_truth = ground_truth.to(device)           \n",
    "            ground_truth_k = torch.fft.fftshift(torch.fft.fft2(ground_truth))  # Fully-sampled k-space\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(undersampled, undersampled, mask=torch.tensor(mask_2d, dtype=torch.float32).to(device))\n",
    "            output_k = torch.fft.fftshift(torch.fft.fft2(output))  # Fully-sampled k-space\n",
    "            \n",
    "            ground_truth_k = ground_truth_k / torch.abs(ground_truth_k).max()\n",
    "            output_k = output_k / torch.abs(output_k).max()\n",
    "\n",
    "            loss = loss_fn(output, ground_truth)\n",
    "            #loss = loss_fn(torch.log1p(output_k.abs()), torch.log1p(ground_truth_k.abs()))\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_loss /= len(train_dataloader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Training Loss: {train_loss:.10f}\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for undersampled, ground_truth in val_dataloader:\n",
    "                undersampled = undersampled.to(device)\n",
    "                ground_truth = ground_truth.to(device)\n",
    "                ground_truth_k = torch.fft.fftshift(torch.fft.fft2(ground_truth))  # Fully-sampled k-space               \n",
    "                output = model(undersampled, undersampled, mask=torch.tensor(mask_2d, dtype=torch.float32).to(device))\n",
    "                output_k = torch.fft.fftshift(torch.fft.fft2(output))  # Reconstructed k-space\n",
    "                \n",
    "                ground_truth_k = ground_truth_k / torch.abs(ground_truth_k).max()\n",
    "                output_k = output_k / torch.abs(output_k).max()            \n",
    "                \n",
    "                loss = loss_fn(output, ground_truth)\n",
    "                #loss = loss_fn(torch.log1p(output_k.abs()), torch.log1p(ground_truth_k.abs()))# \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_dataloader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {val_loss:.10f}\")\n",
    "\n",
    "            \n",
    "def save_results_as_numpy(undersampled, output, ground_truth, index, save_dir=\"results\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "    \n",
    "    np.save(os.path.join(save_dir, f\"undersampled_{index}.npy\"), undersampled)\n",
    "    np.save(os.path.join(save_dir, f\"output_{index}.npy\"), output)\n",
    "    np.save(os.path.join(save_dir, f\"ground_truth_{index}.npy\"), ground_truth)\n",
    "    #print(f\"Results for sample {index} saved in {save_dir}\")\n",
    "\n",
    "            \n",
    "def process_and_save_all_test_images(model, test_dataloader, device, save_dir=\"results\"):\n",
    "    model.eval()\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Ensure the save directory exists\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (undersampled, ground_truth) in enumerate(test_dataloader):\n",
    "            #print(f\"Processing test sample {i}\")\n",
    "\n",
    "            # Move data to device\n",
    "            undersampled = undersampled.to(device)\n",
    "            ground_truth = ground_truth.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(undersampled, undersampled, mask=torch.tensor(mask_2d, dtype=torch.float32).to(device))\n",
    "\n",
    "            # Convert tensors to numpy arrays\n",
    "            undersampled_np = undersampled[0].cpu().numpy()  # Move to CPU and convert to NumPy\n",
    "            output_np = output[0].cpu().numpy()  # Move to CPU and convert to NumPy\n",
    "            ground_truth_np = ground_truth[0].cpu().numpy()  # Move to CPU and convert to NumPy\n",
    "\n",
    "            # Save the results\n",
    "            save_results_as_numpy(undersampled_np, output_np, ground_truth_np, index=i, save_dir=save_dir)\n",
    "\n",
    "            #print(f\"Saved test sample {i} to {save_dir}\")\n",
    "            \n",
    "            # Free memory\n",
    "            del undersampled, ground_truth, output\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "        # Define paths\n",
    "    undersampled_folder = r\"D:\\Class Project\\209\\brain_multicoil_train_batch_1\\noisy_images_0.5\"\n",
    "    ground_truth_folder = r\"D:\\Class Project\\209\\brain_multicoil_train_batch_1\\ground_truth_images\"\n",
    "    \n",
    "    # Create mask\n",
    "    height, width = 640, 320  # Example dimensions (adjust as needed)\n",
    "    np.random.seed(50)\n",
    "    mask_2d = create_random_mask(height, width, center_fraction=0.2, undersample_fraction=0.5, seed=50)# this need to match the mask used to generate data!!!\n",
    "    \n",
    "    # Define dataset\n",
    "    dataset = MRIDataset(undersampled_folder, ground_truth_folder, mask_2d)\n",
    "    \n",
    "    # Split dataset\n",
    "    train_size = int(0.7 * len(dataset))\n",
    "    val_size = int(0.2 * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = VSQPReconstructionNetwork(num_layers=5)\n",
    "    \n",
    "    # Train model\n",
    "    train_network(model, train_dataloader, val_dataloader, num_epochs=20, learning_rate=0.001, device=device)\n",
    "    \n",
    "    # Test and visualize\n",
    "    #test_network(model, test_dataloader, device=device)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    #evaluate_network(model, dataloader, device=device)\n",
    "    process_and_save_all_test_images(model, test_dataloader, device=device, save_dir=\"results\")\n",
    "    print('complete!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
