{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split\n",
    "import cv2\n",
    "import pywt\n",
    "\n",
    "\n",
    "# Helper function for loading TIFF images\n",
    "def load_tiff_images(folder_path):\n",
    "    images = []\n",
    "    for file_name in sorted(os.listdir(folder_path)):\n",
    "        if file_name.endswith(\".tif\") or file_name.endswith(\".tiff\"):\n",
    "            img = Image.open(os.path.join(folder_path, file_name))\n",
    "            images.append(np.array(img, dtype=np.float32))\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, undersampled_folder, ground_truth_folder, mask):\n",
    "        self.undersampled_images = load_tiff_images(undersampled_folder)\n",
    "        self.ground_truth_images = load_tiff_images(ground_truth_folder)\n",
    "        self.mask = mask\n",
    "        \n",
    "        # Ensure consistent shapes by filtering\n",
    "        self.filtered_undersampled_images = []\n",
    "        self.filtered_ground_truth_images = []\n",
    "\n",
    "        for undersampled, ground_truth in zip(self.undersampled_images, self.ground_truth_images):\n",
    "            if ground_truth.shape == (640, 320) and undersampled.shape == (640, 320):\n",
    "                self.filtered_undersampled_images.append(undersampled)\n",
    "                self.filtered_ground_truth_images.append(ground_truth)\n",
    "\n",
    "        print(f\"Filtered dataset contains {len(self.filtered_undersampled_images)} samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filtered_undersampled_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        undersampled = self.filtered_undersampled_images[idx]\n",
    "        ground_truth = self.filtered_ground_truth_images[idx]\n",
    "        \n",
    "        # Normalize images\n",
    "        undersampled = undersampled / np.max(undersampled)\n",
    "        ground_truth = ground_truth / np.max(ground_truth)\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(undersampled, dtype=torch.float32),  # Apply mask\n",
    "            torch.tensor(ground_truth, dtype=torch.float32),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_mask(height, width, center_fraction=0.1, undersample_fraction=0.1, seed=50):#at least 0.5\n",
    "    \"\"\"\n",
    "    Create a 2D random mask with a fully sampled center region.\n",
    "\n",
    "    Args:\n",
    "        height: The height of the mask (frequency encoding dimension).\n",
    "        width: The width of the mask (phase encoding dimension).\n",
    "        center_fraction: Fraction of the image to be fully sampled at the center.\n",
    "        undersample_fraction: Fraction of the remaining k-space to be sampled randomly.\n",
    "\n",
    "    Returns:\n",
    "        mask_2d: A binary mask with the same shape as the k-space data.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)  # Set the random seed locally\n",
    "        mask = np.zeros((height, width))\n",
    "    \n",
    "    # Fully sample the center region\n",
    "    center_height = int(height * center_fraction)\n",
    "    center_width = int(width * center_fraction)\n",
    "    center_start_h = (height - center_height) // 2\n",
    "    center_start_w = (width - center_width) // 2\n",
    "    mask[center_start_h:center_start_h + center_height, center_start_w:center_start_w + center_width] = 1\n",
    "    \n",
    "    # Randomly sample the remaining k-space\n",
    "    remaining_mask = np.random.choice([0, 1], size=(height, width), p=[undersample_fraction, 1-undersample_fraction])\n",
    "    mask = np.maximum(mask, remaining_mask)  # Ensure the center is fully sampled\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAE_4_ReIm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DAE_4_ReIm, self).__init__()\n",
    "        # Update input channels to 2 for real and imaginary parts\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(2, 16, kernel_size=3, stride=1, padding=1),  # Input channels = 2\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 2, kernel_size=3, stride=1, padding=1)  # Output channels = 2\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAE_Bottleneck_ReIm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DAE_Bottleneck_ReIm, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(2, 32, kernel_size=3, stride=1, padding=1),  # Input channels = 2 (real + imaginary)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 2, kernel_size=3, stride=1, padding=1),  # Output channels = 2 (real + imaginary)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Input tensor with 2 channels (real and imaginary parts).\n",
    "        \"\"\"\n",
    "        encoded = self.encoder(x)\n",
    "        bottleneck = self.bottleneck(encoded)\n",
    "        decoded = self.decoder(bottleneck)\n",
    "        return decoded  # Output tensor with 2 channels (real and imaginary parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDAE_ReIm(nn.Module):\n",
    "    def __init__(self, input_channels=2, base_filters=64):  # Set input_channels=2 for real and imaginary\n",
    "        super(UNetDAE_ReIm, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = self.conv_block(input_channels, base_filters)\n",
    "        self.enc2 = self.conv_block(base_filters, base_filters * 2)\n",
    "        self.enc3 = self.conv_block(base_filters * 2, base_filters * 4)\n",
    "        self.enc4 = self.conv_block(base_filters * 4, base_filters * 8)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.conv_block(base_filters * 8, base_filters * 16)\n",
    "        \n",
    "        # Decoder\n",
    "        self.up4 = nn.ConvTranspose2d(base_filters * 16, base_filters * 8, kernel_size=2, stride=2)\n",
    "        self.dec4 = self.conv_block(base_filters * 16, base_filters * 8)\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose2d(base_filters * 8, base_filters * 4, kernel_size=2, stride=2)\n",
    "        self.dec3 = self.conv_block(base_filters * 8, base_filters * 4)\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(base_filters * 4, base_filters * 2, kernel_size=2, stride=2)\n",
    "        self.dec2 = self.conv_block(base_filters * 4, base_filters * 2)\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(base_filters * 2, base_filters, kernel_size=2, stride=2)\n",
    "        self.dec1 = self.conv_block(base_filters * 2, base_filters)\n",
    "        \n",
    "        # Final output layer (2 channels for real and imaginary parts)\n",
    "        self.final = nn.Conv2d(base_filters, input_channels, kernel_size=1)\n",
    "\n",
    "        # Pooling\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(self.pool(enc1))\n",
    "        enc3 = self.enc3(self.pool(enc2))\n",
    "        enc4 = self.enc4(self.pool(enc3))\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(self.pool(enc4))\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        dec4 = self.dec4(torch.cat([self.up4(bottleneck), enc4], dim=1))\n",
    "        dec3 = self.dec3(torch.cat([self.up3(dec4), enc3], dim=1))\n",
    "        dec2 = self.dec2(torch.cat([self.up2(dec3), enc2], dim=1))\n",
    "        dec1 = self.dec1(torch.cat([self.up1(dec2), enc1], dim=1))\n",
    "        \n",
    "        return self.final(dec1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset contains 2049 samples.\n",
      "2049\n",
      "1434\n",
      "359\n",
      "Epoch [1/5], Training Loss: 0.0015474456\n",
      "Epoch [1/5], Validation Loss: 0.0007376645\n",
      "Epoch [2/5], Training Loss: 0.0006488411\n",
      "Epoch [2/5], Validation Loss: 0.0006449894\n",
      "Epoch [3/5], Training Loss: 0.0005740987\n",
      "Epoch [3/5], Validation Loss: 0.0006079651\n",
      "Epoch [4/5], Training Loss: 0.0005221085\n",
      "Epoch [4/5], Validation Loss: 0.0005763932\n",
      "Epoch [5/5], Training Loss: 0.0005032187\n",
      "Epoch [5/5], Validation Loss: 0.0005739719\n",
      "complete!\n"
     ]
    }
   ],
   "source": [
    "class VSQPReconstructionNetwork(nn.Module):\n",
    "    def __init__(self, num_layers=5):\n",
    "        super(VSQPReconstructionNetwork, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.mu = nn.Parameter(torch.tensor(0.1))  # Learnable penalty parameter\n",
    "        self.regularization_net = DAE_4_ReIm()  # Input channels = 2 for real and imaginary\n",
    "\n",
    "    def forward(self, x_init, y, mask):\n",
    "        \"\"\"\n",
    "        x_init: Initial estimate of the image (image space)\n",
    "        y: Undersampled k-space data (in image space, transformed to k-space)\n",
    "        mask: Sampling mask (applied in k-space)\n",
    "        \"\"\"\n",
    "        x = x_init\n",
    "        y_k = torch.fft.fftshift(torch.fft.fft2(torch.fft.ifftshift(y)))  # Transform y to k-space and apply fftshift\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            # Split into real and imaginary parts\n",
    "            x = x +0j\n",
    "            x_real = x.real\n",
    "            x_imag = x.imag\n",
    "            x_complex = torch.stack((x_real, x_imag), dim=1)  # Shape: [batch, 2, height, width]\n",
    "            \n",
    "            # Regularization step using the UNet\n",
    "            z_complex = self.regularization_net(x_complex)\n",
    "            z = z_complex[:, 0] + 1j * z_complex[:, 1]  # Recombine real and imaginary\n",
    "            \n",
    "            # Data consistency step\n",
    "            x_k = torch.fft.fftshift(torch.fft.fft2(torch.fft.ifftshift(x)))  # Transform x to k-space and apply fftshift\n",
    "            Ax = mask * x_k  # Apply k-space mask\n",
    "            residual_k = y_k - Ax\n",
    "            residual = torch.fft.fftshift(torch.fft.ifft2(torch.fft.ifftshift(residual_k)))  # Inverse FT to image space\n",
    "            \n",
    "            # Combine regularized and data consistency updates\n",
    "            x = self.mu * z + residual\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_network(model, train_dataloader, val_dataloader, num_epochs, learning_rate, device):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for undersampled, ground_truth in train_dataloader:\n",
    "            undersampled = undersampled.to(device)\n",
    "            ground_truth = ground_truth.to(device)           \n",
    "            ground_truth_k = torch.fft.fftshift(torch.fft.fft2(ground_truth))  # Fully-sampled k-space\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(undersampled, undersampled, mask=torch.tensor(mask_2d, dtype=torch.float32).to(device))\n",
    "            output_k = torch.fft.fftshift(torch.fft.fft2(output))  # Fully-sampled k-space\n",
    "            \n",
    "            ground_truth_k = ground_truth_k / torch.abs(ground_truth_k).max()\n",
    "            output_k = output_k / torch.abs(output_k).max()\n",
    "            \n",
    "            output = torch.abs(output)\n",
    "            #output = (output - torch.min(output)) / (torch.max(output) - torch.min(output))\n",
    "            \n",
    "            ground_truth = torch.abs(ground_truth)\n",
    "            #ground_truth = (ground_truth - torch.min(ground_truth)) / (torch.max(ground_truth) - torch.min(ground_truth))\n",
    "\n",
    "            loss = loss_fn(torch.abs(output), torch.abs(ground_truth))\n",
    "            #loss = loss_fn(torch.log1p(output_k.abs()), torch.log1p(ground_truth_k.abs()))\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_loss /= len(train_dataloader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Training Loss: {train_loss:.10f}\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for undersampled, ground_truth in val_dataloader:\n",
    "                undersampled = undersampled.to(device)\n",
    "                ground_truth = ground_truth.to(device)\n",
    "                ground_truth_k = torch.fft.fftshift(torch.fft.fft2(ground_truth))  # Fully-sampled k-space               \n",
    "                output = model(undersampled, undersampled, mask=torch.tensor(mask_2d, dtype=torch.float32).to(device))\n",
    "                output_k = torch.fft.fftshift(torch.fft.fft2(output))  # Reconstructed k-space\n",
    "                \n",
    "                ground_truth_k = ground_truth_k / torch.abs(ground_truth_k).max()\n",
    "                output_k = output_k / torch.abs(output_k).max()\n",
    "                \n",
    "                output = torch.abs(output)\n",
    "                #output = (output - torch.min(output)) / (torch.max(output) - torch.min(output))\n",
    "\n",
    "                \n",
    "                ground_truth = torch.abs(ground_truth)\n",
    "                #ground_truth = (ground_truth - torch.min(ground_truth)) / (torch.max(ground_truth) - torch.min(ground_truth))\n",
    "\n",
    "                loss = loss_fn(torch.abs(output), torch.abs(ground_truth))\n",
    "                #loss = loss_fn(torch.log1p(output_k.abs()), torch.log1p(ground_truth_k.abs()))# \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_dataloader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {val_loss:.10f}\")\n",
    "\n",
    "            \n",
    "def save_results_as_numpy(undersampled, output, ground_truth, index, save_dir=\"results\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "    \n",
    "    np.save(os.path.join(save_dir, f\"undersampled_{index}.npy\"), undersampled)\n",
    "    np.save(os.path.join(save_dir, f\"output_{index}.npy\"), output)\n",
    "    np.save(os.path.join(save_dir, f\"ground_truth_{index}.npy\"), ground_truth)\n",
    "    #print(f\"Results for sample {index} saved in {save_dir}\")\n",
    "\n",
    "            \n",
    "def process_and_save_all_test_images(model, test_dataloader, device, save_dir=\"results\"):\n",
    "    model.eval()\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Ensure the save directory exists\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (undersampled, ground_truth) in enumerate(test_dataloader):\n",
    "            #print(f\"Processing test sample {i}\")\n",
    "\n",
    "            # Move data to device\n",
    "            undersampled = undersampled.to(device)\n",
    "            ground_truth = ground_truth.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(undersampled, undersampled, mask=torch.tensor(mask_2d, dtype=torch.float32).to(device))\n",
    "\n",
    "            # Convert tensors to numpy arrays\n",
    "            undersampled_np = undersampled[0].cpu().numpy()  # Move to CPU and convert to NumPy\n",
    "            output_np = output[0].cpu().numpy()  # Move to CPU and convert to NumPy\n",
    "            ground_truth_np = ground_truth[0].cpu().numpy()  # Move to CPU and convert to NumPy\n",
    "\n",
    "            # Save the results\n",
    "            save_results_as_numpy(undersampled_np, output_np, ground_truth_np, index=i, save_dir=save_dir)\n",
    "\n",
    "            #print(f\"Saved test sample {i} to {save_dir}\")\n",
    "            \n",
    "            # Free memory\n",
    "            del undersampled, ground_truth, output\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "        # Define paths\n",
    "    undersampled_folder = r\"D:\\Class Project\\209\\brain_multicoil_train_batch_1\\noisy_images_0.75\"\n",
    "    ground_truth_folder = r\"D:\\Class Project\\209\\brain_multicoil_train_batch_1\\ground_truth_images\"\n",
    "    \n",
    "    # Create mask\n",
    "    height, width = 640, 320  # Example dimensions (adjust as needed)\n",
    "    np.random.seed(50)\n",
    "    mask_2d = create_random_mask(height, width, center_fraction=0.2, undersample_fraction=0.75, seed=50)# this need to match the mask used to generate data!!!\n",
    "    \n",
    "    # Define dataset\n",
    "    dataset = MRIDataset(undersampled_folder, ground_truth_folder, mask_2d)\n",
    "    print(len(dataset))\n",
    "    \n",
    "    # Split dataset\n",
    "    train_size = int(0.7 * len(dataset))\n",
    "    val_size = int(0.2 * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "    print(len(train_dataset))\n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    print(len(train_dataloader))\n",
    "    # Initialize model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = VSQPReconstructionNetwork(num_layers=5)\n",
    "    \n",
    "    # Train model\n",
    "    train_network(model, train_dataloader, val_dataloader, num_epochs=5, learning_rate=0.001, device=device)\n",
    "    \n",
    "    # Test and visualize\n",
    "    #test_network(model, test_dataloader, device=device)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    #evaluate_network(model, dataloader, device=device)\n",
    "    process_and_save_all_test_images(model, test_dataloader, device=device, save_dir=\"results\")\n",
    "    print('complete!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
