{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split\n",
    "import cv2\n",
    "import pywt\n",
    "\n",
    "\n",
    "# Helper function for loading TIFF images\n",
    "def load_tiff_images(folder_path):\n",
    "    images = []\n",
    "    for file_name in sorted(os.listdir(folder_path)):\n",
    "        if file_name.endswith(\".tif\") or file_name.endswith(\".tiff\"):\n",
    "            img = Image.open(os.path.join(folder_path, file_name))\n",
    "            images.append(np.array(img, dtype=np.float32))\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, undersampled_folder, ground_truth_folder, mask):\n",
    "        self.undersampled_images = load_tiff_images(undersampled_folder)\n",
    "        self.ground_truth_images = load_tiff_images(ground_truth_folder)\n",
    "        self.mask = mask\n",
    "        \n",
    "        # Ensure consistent shapes by filtering\n",
    "        self.filtered_undersampled_images = []\n",
    "        self.filtered_ground_truth_images = []\n",
    "\n",
    "        for undersampled, ground_truth in zip(self.undersampled_images, self.ground_truth_images):\n",
    "            if ground_truth.shape == (640, 320) and undersampled.shape == (640, 320):\n",
    "                self.filtered_undersampled_images.append(undersampled)\n",
    "                self.filtered_ground_truth_images.append(ground_truth)\n",
    "\n",
    "        print(f\"Filtered dataset contains {len(self.filtered_undersampled_images)} samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filtered_undersampled_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        undersampled = self.filtered_undersampled_images[idx]\n",
    "        ground_truth = self.filtered_ground_truth_images[idx]\n",
    "        \n",
    "        # Normalize images\n",
    "        undersampled = undersampled / np.max(undersampled)\n",
    "        ground_truth = ground_truth / np.max(ground_truth)\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(undersampled, dtype=torch.float32),  # Apply mask\n",
    "            torch.tensor(ground_truth, dtype=torch.float32),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset contains 2049 samples.\n",
      "Epoch [1/20], Training Loss: 0.0000009631\n",
      "Epoch [1/20], Validation Loss: 0.0000009453\n",
      "Epoch [2/20], Training Loss: 0.0000008754\n",
      "Epoch [2/20], Validation Loss: 0.0000008326\n",
      "Epoch [3/20], Training Loss: 0.0000007003\n",
      "Epoch [3/20], Validation Loss: 0.0000005705\n",
      "Epoch [4/20], Training Loss: 0.0000004329\n",
      "Epoch [4/20], Validation Loss: 0.0000003511\n",
      "Epoch [5/20], Training Loss: 0.0000003467\n",
      "Epoch [5/20], Validation Loss: 0.0000003441\n",
      "Epoch [6/20], Training Loss: 0.0000003468\n",
      "Epoch [6/20], Validation Loss: 0.0000003437\n",
      "Epoch [7/20], Training Loss: 0.0000003436\n",
      "Epoch [7/20], Validation Loss: 0.0000003437\n",
      "Epoch [8/20], Training Loss: 0.0000003429\n",
      "Epoch [8/20], Validation Loss: 0.0000003438\n",
      "Epoch [9/20], Training Loss: 0.0000003419\n",
      "Epoch [9/20], Validation Loss: 0.0000003438\n",
      "Epoch [10/20], Training Loss: 0.0000003475\n",
      "Epoch [10/20], Validation Loss: 0.0000003439\n",
      "Epoch [11/20], Training Loss: 0.0000003480\n",
      "Epoch [11/20], Validation Loss: 0.0000003437\n",
      "Epoch [12/20], Training Loss: 0.0000003436\n",
      "Epoch [12/20], Validation Loss: 0.0000003438\n",
      "Epoch [13/20], Training Loss: 0.0000003500\n",
      "Epoch [13/20], Validation Loss: 0.0000003444\n",
      "Epoch [14/20], Training Loss: 0.0000003502\n",
      "Epoch [14/20], Validation Loss: 0.0000003438\n",
      "Epoch [15/20], Training Loss: 0.0000003462\n",
      "Epoch [15/20], Validation Loss: 0.0000003438\n",
      "Epoch [16/20], Training Loss: 0.0000003465\n",
      "Epoch [16/20], Validation Loss: 0.0000003438\n",
      "Epoch [17/20], Training Loss: 0.0000003472\n",
      "Epoch [17/20], Validation Loss: 0.0000003437\n",
      "Epoch [18/20], Training Loss: 0.0000003467\n",
      "Epoch [18/20], Validation Loss: 0.0000003437\n",
      "Epoch [19/20], Training Loss: 0.0000003478\n",
      "Epoch [19/20], Validation Loss: 0.0000003438\n",
      "Epoch [20/20], Training Loss: 0.0000003495\n",
      "Epoch [20/20], Validation Loss: 0.0000003441\n",
      "Processing test sample 0\n",
      "Results for sample 0 saved in results\n",
      "Processing test sample 1\n",
      "Results for sample 1 saved in results\n",
      "Processing test sample 2\n",
      "Results for sample 2 saved in results\n",
      "Processing test sample 3\n",
      "Results for sample 3 saved in results\n",
      "Processing test sample 4\n",
      "Results for sample 4 saved in results\n",
      "Processing test sample 5\n",
      "Results for sample 5 saved in results\n",
      "Processing test sample 6\n",
      "Results for sample 6 saved in results\n",
      "Processing test sample 7\n",
      "Results for sample 7 saved in results\n",
      "Processing test sample 8\n",
      "Results for sample 8 saved in results\n",
      "Processing test sample 9\n",
      "Results for sample 9 saved in results\n",
      "Processing test sample 10\n",
      "Results for sample 10 saved in results\n",
      "Processing test sample 11\n",
      "Results for sample 11 saved in results\n",
      "Processing test sample 12\n",
      "Results for sample 12 saved in results\n",
      "Processing test sample 13\n",
      "Results for sample 13 saved in results\n",
      "Processing test sample 14\n",
      "Results for sample 14 saved in results\n",
      "Processing test sample 15\n",
      "Results for sample 15 saved in results\n",
      "Processing test sample 16\n",
      "Results for sample 16 saved in results\n",
      "Processing test sample 17\n",
      "Results for sample 17 saved in results\n",
      "Processing test sample 18\n",
      "Results for sample 18 saved in results\n",
      "Processing test sample 19\n",
      "Results for sample 19 saved in results\n",
      "Processing test sample 20\n",
      "Results for sample 20 saved in results\n",
      "Processing test sample 21\n",
      "Results for sample 21 saved in results\n",
      "Processing test sample 22\n",
      "Results for sample 22 saved in results\n",
      "Processing test sample 23\n",
      "Results for sample 23 saved in results\n",
      "Processing test sample 24\n",
      "Results for sample 24 saved in results\n",
      "Processing test sample 25\n",
      "Results for sample 25 saved in results\n",
      "Processing test sample 26\n",
      "Results for sample 26 saved in results\n",
      "Processing test sample 27\n",
      "Results for sample 27 saved in results\n",
      "Processing test sample 28\n",
      "Results for sample 28 saved in results\n",
      "Processing test sample 29\n",
      "Results for sample 29 saved in results\n",
      "Processing test sample 30\n",
      "Results for sample 30 saved in results\n",
      "Processing test sample 31\n",
      "Results for sample 31 saved in results\n",
      "Processing test sample 32\n",
      "Results for sample 32 saved in results\n",
      "Processing test sample 33\n",
      "Results for sample 33 saved in results\n",
      "Processing test sample 34\n",
      "Results for sample 34 saved in results\n",
      "Processing test sample 35\n",
      "Results for sample 35 saved in results\n",
      "Processing test sample 36\n",
      "Results for sample 36 saved in results\n",
      "Processing test sample 37\n",
      "Results for sample 37 saved in results\n",
      "Processing test sample 38\n",
      "Results for sample 38 saved in results\n",
      "Processing test sample 39\n",
      "Results for sample 39 saved in results\n",
      "Processing test sample 40\n",
      "Results for sample 40 saved in results\n",
      "Processing test sample 41\n",
      "Results for sample 41 saved in results\n",
      "Processing test sample 42\n",
      "Results for sample 42 saved in results\n",
      "Processing test sample 43\n",
      "Results for sample 43 saved in results\n",
      "Processing test sample 44\n",
      "Results for sample 44 saved in results\n",
      "Processing test sample 45\n",
      "Results for sample 45 saved in results\n",
      "Processing test sample 46\n",
      "Results for sample 46 saved in results\n",
      "Processing test sample 47\n",
      "Results for sample 47 saved in results\n",
      "Processing test sample 48\n",
      "Results for sample 48 saved in results\n",
      "Processing test sample 49\n",
      "Results for sample 49 saved in results\n",
      "Processing test sample 50\n",
      "Results for sample 50 saved in results\n",
      "Processing test sample 51\n",
      "Results for sample 51 saved in results\n",
      "Processing test sample 52\n",
      "Results for sample 52 saved in results\n",
      "Processing test sample 53\n",
      "Results for sample 53 saved in results\n",
      "Processing test sample 54\n",
      "Results for sample 54 saved in results\n",
      "Processing test sample 55\n",
      "Results for sample 55 saved in results\n",
      "Processing test sample 56\n",
      "Results for sample 56 saved in results\n",
      "Processing test sample 57\n",
      "Results for sample 57 saved in results\n",
      "Processing test sample 58\n",
      "Results for sample 58 saved in results\n",
      "Processing test sample 59\n",
      "Results for sample 59 saved in results\n",
      "Processing test sample 60\n",
      "Results for sample 60 saved in results\n",
      "Processing test sample 61\n",
      "Results for sample 61 saved in results\n",
      "Processing test sample 62\n",
      "Results for sample 62 saved in results\n",
      "Processing test sample 63\n",
      "Results for sample 63 saved in results\n",
      "Processing test sample 64\n",
      "Results for sample 64 saved in results\n",
      "Processing test sample 65\n",
      "Results for sample 65 saved in results\n",
      "Processing test sample 66\n",
      "Results for sample 66 saved in results\n",
      "Processing test sample 67\n",
      "Results for sample 67 saved in results\n",
      "Processing test sample 68\n",
      "Results for sample 68 saved in results\n",
      "Processing test sample 69\n",
      "Results for sample 69 saved in results\n",
      "Processing test sample 70\n",
      "Results for sample 70 saved in results\n",
      "Processing test sample 71\n",
      "Results for sample 71 saved in results\n",
      "Processing test sample 72\n",
      "Results for sample 72 saved in results\n",
      "Processing test sample 73\n",
      "Results for sample 73 saved in results\n",
      "Processing test sample 74\n",
      "Results for sample 74 saved in results\n",
      "Processing test sample 75\n",
      "Results for sample 75 saved in results\n",
      "Processing test sample 76\n",
      "Results for sample 76 saved in results\n",
      "Processing test sample 77\n",
      "Results for sample 77 saved in results\n",
      "Processing test sample 78\n",
      "Results for sample 78 saved in results\n",
      "Processing test sample 79\n",
      "Results for sample 79 saved in results\n",
      "Processing test sample 80\n",
      "Results for sample 80 saved in results\n",
      "Processing test sample 81\n",
      "Results for sample 81 saved in results\n",
      "Processing test sample 82\n",
      "Results for sample 82 saved in results\n",
      "Processing test sample 83\n",
      "Results for sample 83 saved in results\n",
      "Processing test sample 84\n",
      "Results for sample 84 saved in results\n",
      "Processing test sample 85\n",
      "Results for sample 85 saved in results\n",
      "Processing test sample 86\n",
      "Results for sample 86 saved in results\n",
      "Processing test sample 87\n",
      "Results for sample 87 saved in results\n",
      "Processing test sample 88\n",
      "Results for sample 88 saved in results\n",
      "Processing test sample 89\n",
      "Results for sample 89 saved in results\n",
      "Processing test sample 90\n",
      "Results for sample 90 saved in results\n",
      "Processing test sample 91\n",
      "Results for sample 91 saved in results\n",
      "Processing test sample 92\n",
      "Results for sample 92 saved in results\n",
      "Processing test sample 93\n",
      "Results for sample 93 saved in results\n",
      "Processing test sample 94\n",
      "Results for sample 94 saved in results\n",
      "Processing test sample 95\n",
      "Results for sample 95 saved in results\n",
      "Processing test sample 96\n",
      "Results for sample 96 saved in results\n",
      "Processing test sample 97\n",
      "Results for sample 97 saved in results\n",
      "Processing test sample 98\n",
      "Results for sample 98 saved in results\n",
      "Processing test sample 99\n",
      "Results for sample 99 saved in results\n",
      "Processing test sample 100\n",
      "Results for sample 100 saved in results\n",
      "Processing test sample 101\n",
      "Results for sample 101 saved in results\n",
      "Processing test sample 102\n",
      "Results for sample 102 saved in results\n",
      "Processing test sample 103\n",
      "Results for sample 103 saved in results\n",
      "Processing test sample 104\n",
      "Results for sample 104 saved in results\n",
      "Processing test sample 105\n",
      "Results for sample 105 saved in results\n",
      "Processing test sample 106\n",
      "Results for sample 106 saved in results\n",
      "Processing test sample 107\n",
      "Results for sample 107 saved in results\n",
      "Processing test sample 108\n",
      "Results for sample 108 saved in results\n",
      "Processing test sample 109\n",
      "Results for sample 109 saved in results\n",
      "Processing test sample 110\n",
      "Results for sample 110 saved in results\n",
      "Processing test sample 111\n",
      "Results for sample 111 saved in results\n",
      "Processing test sample 112\n",
      "Results for sample 112 saved in results\n",
      "Processing test sample 113\n",
      "Results for sample 113 saved in results\n",
      "Processing test sample 114\n",
      "Results for sample 114 saved in results\n",
      "Processing test sample 115\n",
      "Results for sample 115 saved in results\n",
      "Processing test sample 116\n",
      "Results for sample 116 saved in results\n",
      "Processing test sample 117\n",
      "Results for sample 117 saved in results\n",
      "Processing test sample 118\n",
      "Results for sample 118 saved in results\n",
      "Processing test sample 119\n",
      "Results for sample 119 saved in results\n",
      "Processing test sample 120\n",
      "Results for sample 120 saved in results\n",
      "Processing test sample 121\n",
      "Results for sample 121 saved in results\n",
      "Processing test sample 122\n",
      "Results for sample 122 saved in results\n",
      "Processing test sample 123\n",
      "Results for sample 123 saved in results\n",
      "Processing test sample 124\n",
      "Results for sample 124 saved in results\n",
      "Processing test sample 125\n",
      "Results for sample 125 saved in results\n",
      "Processing test sample 126\n",
      "Results for sample 126 saved in results\n",
      "Processing test sample 127\n",
      "Results for sample 127 saved in results\n",
      "Processing test sample 128\n",
      "Results for sample 128 saved in results\n",
      "Processing test sample 129\n",
      "Results for sample 129 saved in results\n",
      "Processing test sample 130\n",
      "Results for sample 130 saved in results\n",
      "Processing test sample 131\n",
      "Results for sample 131 saved in results\n",
      "Processing test sample 132\n",
      "Results for sample 132 saved in results\n",
      "Processing test sample 133\n",
      "Results for sample 133 saved in results\n",
      "Processing test sample 134\n",
      "Results for sample 134 saved in results\n",
      "Processing test sample 135\n",
      "Results for sample 135 saved in results\n",
      "Processing test sample 136\n",
      "Results for sample 136 saved in results\n",
      "Processing test sample 137\n",
      "Results for sample 137 saved in results\n",
      "Processing test sample 138\n",
      "Results for sample 138 saved in results\n",
      "Processing test sample 139\n",
      "Results for sample 139 saved in results\n",
      "Processing test sample 140\n",
      "Results for sample 140 saved in results\n",
      "Processing test sample 141\n",
      "Results for sample 141 saved in results\n",
      "Processing test sample 142\n",
      "Results for sample 142 saved in results\n",
      "Processing test sample 143\n",
      "Results for sample 143 saved in results\n",
      "Processing test sample 144\n",
      "Results for sample 144 saved in results\n",
      "Processing test sample 145\n",
      "Results for sample 145 saved in results\n",
      "Processing test sample 146\n",
      "Results for sample 146 saved in results\n",
      "Processing test sample 147\n",
      "Results for sample 147 saved in results\n",
      "Processing test sample 148\n",
      "Results for sample 148 saved in results\n",
      "Processing test sample 149\n",
      "Results for sample 149 saved in results\n",
      "Processing test sample 150\n",
      "Results for sample 150 saved in results\n",
      "Processing test sample 151\n",
      "Results for sample 151 saved in results\n",
      "Processing test sample 152\n",
      "Results for sample 152 saved in results\n",
      "Processing test sample 153\n",
      "Results for sample 153 saved in results\n",
      "Processing test sample 154\n",
      "Results for sample 154 saved in results\n",
      "Processing test sample 155\n",
      "Results for sample 155 saved in results\n",
      "Processing test sample 156\n",
      "Results for sample 156 saved in results\n",
      "Processing test sample 157\n",
      "Results for sample 157 saved in results\n",
      "Processing test sample 158\n",
      "Results for sample 158 saved in results\n",
      "Processing test sample 159\n",
      "Results for sample 159 saved in results\n",
      "Processing test sample 160\n",
      "Results for sample 160 saved in results\n",
      "Processing test sample 161\n",
      "Results for sample 161 saved in results\n",
      "Processing test sample 162\n",
      "Results for sample 162 saved in results\n",
      "Processing test sample 163\n",
      "Results for sample 163 saved in results\n",
      "Processing test sample 164\n",
      "Results for sample 164 saved in results\n",
      "Processing test sample 165\n",
      "Results for sample 165 saved in results\n",
      "Processing test sample 166\n",
      "Results for sample 166 saved in results\n",
      "Processing test sample 167\n",
      "Results for sample 167 saved in results\n",
      "Processing test sample 168\n",
      "Results for sample 168 saved in results\n",
      "Processing test sample 169\n",
      "Results for sample 169 saved in results\n",
      "Processing test sample 170\n",
      "Results for sample 170 saved in results\n",
      "Processing test sample 171\n",
      "Results for sample 171 saved in results\n",
      "Processing test sample 172\n",
      "Results for sample 172 saved in results\n",
      "Processing test sample 173\n",
      "Results for sample 173 saved in results\n",
      "Processing test sample 174\n",
      "Results for sample 174 saved in results\n",
      "Processing test sample 175\n",
      "Results for sample 175 saved in results\n",
      "Processing test sample 176\n",
      "Results for sample 176 saved in results\n",
      "Processing test sample 177\n",
      "Results for sample 177 saved in results\n",
      "Processing test sample 178\n",
      "Results for sample 178 saved in results\n",
      "Processing test sample 179\n",
      "Results for sample 179 saved in results\n",
      "Processing test sample 180\n",
      "Results for sample 180 saved in results\n",
      "Processing test sample 181\n",
      "Results for sample 181 saved in results\n",
      "Processing test sample 182\n",
      "Results for sample 182 saved in results\n",
      "Processing test sample 183\n",
      "Results for sample 183 saved in results\n",
      "Processing test sample 184\n",
      "Results for sample 184 saved in results\n",
      "Processing test sample 185\n",
      "Results for sample 185 saved in results\n",
      "Processing test sample 186\n",
      "Results for sample 186 saved in results\n",
      "Processing test sample 187\n",
      "Results for sample 187 saved in results\n",
      "Processing test sample 188\n",
      "Results for sample 188 saved in results\n",
      "Processing test sample 189\n",
      "Results for sample 189 saved in results\n",
      "Processing test sample 190\n",
      "Results for sample 190 saved in results\n",
      "Processing test sample 191\n",
      "Results for sample 191 saved in results\n",
      "Processing test sample 192\n",
      "Results for sample 192 saved in results\n",
      "Processing test sample 193\n",
      "Results for sample 193 saved in results\n",
      "Processing test sample 194\n",
      "Results for sample 194 saved in results\n",
      "Processing test sample 195\n",
      "Results for sample 195 saved in results\n",
      "Processing test sample 196\n",
      "Results for sample 196 saved in results\n",
      "Processing test sample 197\n",
      "Results for sample 197 saved in results\n",
      "Processing test sample 198\n",
      "Results for sample 198 saved in results\n",
      "Processing test sample 199\n",
      "Results for sample 199 saved in results\n",
      "Processing test sample 200\n",
      "Results for sample 200 saved in results\n",
      "Processing test sample 201\n",
      "Results for sample 201 saved in results\n",
      "Processing test sample 202\n",
      "Results for sample 202 saved in results\n",
      "Processing test sample 203\n",
      "Results for sample 203 saved in results\n",
      "Processing test sample 204\n",
      "Results for sample 204 saved in results\n",
      "Processing test sample 205\n",
      "Results for sample 205 saved in results\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class VSQPReconstructionNetwork(nn.Module):\n",
    "    def __init__(self, num_layers=5):\n",
    "        super(VSQPReconstructionNetwork, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.mu = nn.Parameter(torch.tensor(0.1))  # Learnable penalty parameter\n",
    "        \n",
    "        # Use nn.ParameterList to store the regularization thresholds\n",
    "        self.regularizers = nn.ParameterList([\n",
    "            nn.Parameter(torch.tensor(0.1)) for _ in range(num_layers)  # Thresholds for soft-thresholding\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x_init, y, mask):\n",
    "        \"\"\"\n",
    "        x_init: Initial estimate of the image (image space)\n",
    "        y: Undersampled k-space data (in image space, transformed to k-space)\n",
    "        mask: Sampling mask (applied in k-space)\n",
    "        \"\"\"\n",
    "        x = x_init\n",
    "        y_k = torch.fft.fftshift(torch.fft.fft2(y))  # Transform y to k-space and apply fftshift\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            # Regularization step (proximal operator - soft-thresholding)\n",
    "            z = self.apply_wavelet_soft_threshold(x, self.regularizers[i])\n",
    "\n",
    "            # Data consistency step\n",
    "            # Step 1: Forward model (apply mask in k-space)\n",
    "            x_k = torch.fft.fftshift(torch.fft.fft2(x))  # Transform x to k-space and apply fftshift\n",
    "            Ax = mask * x_k  # Apply k-space mask\n",
    "            \n",
    "            # Step 2: Residual in k-space\n",
    "            residual_k = y_k - Ax\n",
    "            \n",
    "            # Step 3: Backproject residual (to image space)\n",
    "            residual_k_shifted = mask * residual_k  # Apply mask again (if needed)\n",
    "            residual = torch.fft.ifft2(torch.fft.ifftshift(residual_k_shifted)).real  # Inverse FT to image space\n",
    "\n",
    "            # Combine regularized and data consistency updates\n",
    "            x = z + self.mu * residual\n",
    "\n",
    "        return x\n",
    "\n",
    "    def apply_wavelet_soft_threshold(self, x, threshold):\n",
    "        # Detach and move the tensor to CPU before converting to NumPy\n",
    "        x_np = x.detach().cpu().numpy()\n",
    "        \n",
    "        # Wavelet decomposition\n",
    "        coeffs = pywt.wavedec2(x_np, wavelet='haar')\n",
    "        \n",
    "        # Apply soft thresholding to coefficients\n",
    "        thresholded_coeffs = [\n",
    "            pywt.threshold(c, threshold.item(), mode='soft') if isinstance(c, np.ndarray) else c\n",
    "            for c in coeffs\n",
    "        ]\n",
    "        \n",
    "        # Wavelet reconstruction\n",
    "        x_thresholded = pywt.waverec2(thresholded_coeffs, wavelet='haar')\n",
    "        \n",
    "        # Convert back to a PyTorch tensor\n",
    "        return torch.tensor(x_thresholded, device=x.device, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_network(model, train_dataloader, val_dataloader, num_epochs, learning_rate, device):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for undersampled, ground_truth in train_dataloader:\n",
    "            undersampled = undersampled.to(device)\n",
    "            ground_truth = ground_truth.to(device)           \n",
    "            ground_truth_k = torch.fft.fftshift(torch.fft.fft2(ground_truth))  # Fully-sampled k-space\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(undersampled, undersampled, mask=torch.tensor(mask_2d, dtype=torch.float32).to(device))\n",
    "            output_k = torch.fft.fftshift(torch.fft.fft2(output))  # Fully-sampled k-space\n",
    "            \n",
    "            ground_truth_k = ground_truth_k / torch.abs(ground_truth_k).max()\n",
    "            output_k = output_k / torch.abs(output_k).max()\n",
    "\n",
    "            loss = loss_fn(torch.log1p(output_k.abs()), torch.log1p(ground_truth_k.abs()))\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_loss /= len(train_dataloader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Training Loss: {train_loss:.10f}\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for undersampled, ground_truth in val_dataloader:\n",
    "                undersampled = undersampled.to(device)\n",
    "                ground_truth = ground_truth.to(device)\n",
    "                ground_truth_k = torch.fft.fftshift(torch.fft.fft2(ground_truth))  # Fully-sampled k-space               \n",
    "                output = model(undersampled, undersampled, mask=torch.tensor(mask_2d, dtype=torch.float32).to(device))\n",
    "                output_k = torch.fft.fftshift(torch.fft.fft2(output))  # Reconstructed k-space\n",
    "                \n",
    "                ground_truth_k = ground_truth_k / torch.abs(ground_truth_k).max()\n",
    "                output_k = output_k / torch.abs(output_k).max()            \n",
    "                \n",
    "                loss = loss_fn(torch.log1p(output_k.abs()), torch.log1p(ground_truth_k.abs()))\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_dataloader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {val_loss:.10f}\")\n",
    "\n",
    "            \n",
    "def save_results_as_numpy(undersampled, output, ground_truth, index, save_dir=\"results\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "    \n",
    "    np.save(os.path.join(save_dir, f\"undersampled_{index}.npy\"), undersampled)\n",
    "    np.save(os.path.join(save_dir, f\"output_{index}.npy\"), output)\n",
    "    np.save(os.path.join(save_dir, f\"ground_truth_{index}.npy\"), ground_truth)\n",
    "    #print(f\"Results for sample {index} saved in {save_dir}\")\n",
    "\n",
    "            \n",
    "def process_and_save_all_test_images(model, test_dataloader, device, save_dir=\"results\"):\n",
    "    model.eval()\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Ensure the save directory exists\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (undersampled, ground_truth) in enumerate(test_dataloader):\n",
    "            print(f\"Processing test sample {i}\")\n",
    "\n",
    "            # Move data to device\n",
    "            undersampled = undersampled.to(device)\n",
    "            ground_truth = ground_truth.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(undersampled, undersampled, mask=torch.tensor(mask_2d, dtype=torch.float32).to(device))\n",
    "\n",
    "            # Convert tensors to numpy arrays\n",
    "            undersampled_np = undersampled[0].cpu().numpy()  # Move to CPU and convert to NumPy\n",
    "            output_np = output[0].cpu().numpy()  # Move to CPU and convert to NumPy\n",
    "            ground_truth_np = ground_truth[0].cpu().numpy()  # Move to CPU and convert to NumPy\n",
    "\n",
    "            # Save the results\n",
    "            save_results_as_numpy(undersampled_np, output_np, ground_truth_np, index=i, save_dir=save_dir)\n",
    "\n",
    "            #print(f\"Saved test sample {i} to {save_dir}\")\n",
    "            \n",
    "            # Free memory\n",
    "            del undersampled, ground_truth, output\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "        # Define paths\n",
    "    undersampled_folder = r\"D:\\Class Project\\209\\brain_multicoil_train_batch_1\\noisy_images_0.2\"\n",
    "    ground_truth_folder = r\"D:\\Class Project\\209\\brain_multicoil_train_batch_1\\ground_truth_images\"\n",
    "    \n",
    "    # Create mask\n",
    "    height, width = 640, 320  # Example dimensions (adjust as needed)\n",
    "    np.random.seed(50)\n",
    "    mask_2d = np.random.choice([0, 1], size=(height, width), p=[0.1, 0.9])\n",
    "    \n",
    "    # Define dataset\n",
    "    dataset = MRIDataset(undersampled_folder, ground_truth_folder, mask_2d)\n",
    "    \n",
    "    # Split dataset\n",
    "    train_size = int(0.7 * len(dataset))\n",
    "    val_size = int(0.2 * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = VSQPReconstructionNetwork(num_layers=5)\n",
    "    \n",
    "    # Train model\n",
    "    train_network(model, train_dataloader, val_dataloader, num_epochs=20, learning_rate=0.001, device=device)\n",
    "    \n",
    "    # Test and visualize\n",
    "    #test_network(model, test_dataloader, device=device)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    #evaluate_network(model, dataloader, device=device)\n",
    "    process_and_save_all_test_images(model, test_dataloader, device=device, save_dir=\"results\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
